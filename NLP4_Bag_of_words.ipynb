{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP4_Bag_of_words.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP9YvSFPrHEftJnbeeOxrWB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RemyaRS/NLP-Basics/blob/main/NLP4_Bag_of_words.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing all Libraries needed"
      ],
      "metadata": {
        "id": "TB0TMa6vwdSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "T0rJ91aPnbPi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b388cf72-3938-41ab-a473-b08edd2fc75e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning the texts\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem.porter import PorterStemmer#for stemming\n",
        "from nltk.stem import WordNetLemmatizer#for lemmatization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHoftdR8seBL",
        "outputId": "9c4f4a75-3bcc-49b3-9a2b-46ebcb915d68"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = \"\"\"would like to conclude by saying that no matter how much I praise my countryâ€™s achievement, it would always be less. Our country is our motherland and we should always be grateful for whatever we have got from our country. We should participate and put positive efforts in the growth and development of our nation.\n",
        "\n",
        "I would like to request you to make our nation proud, may it be by keeping our vicinity clean, educating and empowering the weaker section of the society, paying our taxes or conserving our natural resources by recycling and reusing. I would like to thank God for giving me this great nation India as my motherland.\"\"\""
      ],
      "metadata": {
        "id": "8lPDxhsywI37"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PorterStemmer()\n",
        "wordnet=WordNetLemmatizer()\n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "corpus = [] #list formed to store all sentences\n",
        "for i in range(len(sentences)):\n",
        "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])#to replace everything other than a-z, A-Z and spaces\n",
        "    review = review.lower()#lower case everything left\n",
        "    review = review.split()#to produce list of words\n",
        "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]#stemming via ps + eleminating stopwords\n",
        "    review = ' '.join(review)#joining all list of words\n",
        "    corpus.append(review)#appending"
      ],
      "metadata": {
        "id": "DTJ0VISlsg8Y"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the Bag of Words model : document matrix\n",
        "from sklearn.feature_extraction.text import CountVectorizer #CountVectorizer is a library\n",
        "cv = CountVectorizer(max_features = 1500) # cv is an object\n",
        "X = cv.fit_transform(corpus).toarray()"
      ],
      "metadata": {
        "id": "DZo-8D5Jst72"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}